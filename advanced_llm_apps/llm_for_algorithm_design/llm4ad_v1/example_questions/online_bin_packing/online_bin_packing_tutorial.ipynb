{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ba1915fced4e72",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tutorial on online bin packing problem\n",
    "## Please open in Colab !!!\n",
    "Five steps:\n",
    "1. Implement a sampler\n",
    "2. Implement an evaluator and prepare a template program\n",
    "3. Choose a method and run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d02b8e9c3ba67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparation: download the project file from GitHub. And update system path."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download code from GitHub."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34937ecb66772bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f419cd674eb4fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "!rm -rf llm4ad\n",
    "!git clone https://github.com/Optima-CityU/llm4ad.git"
   ]
  },
  {
   "cell_type": "code",
   "id": "22453e8153e0934c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:04:13.420343Z",
     "start_time": "2024-11-27T05:04:13.415898Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/content/llm4ad/')\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import llm4ad\n",
    "from llm4ad.task.optimization.online_bin_packing import OBPEvaluation"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "fe47175708cc0a93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Implement an LLM interface\n",
    "The sampler defines the way to use local LLM or LLM API. You should create a new Sampler class by implementing `llm4ad.base.LLM`.\n",
    "- You should implement \"draw_sample\" function, to let the package know how to get a LLM's response by given a prompt.\n",
    "- If you want more acceleration (such as batch inference, multi-threading sampling) you can also override \"draw_samples\" function.\n",
    "- The following example shows a fake sampler, which returns a random function in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051608732d45a2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following example shows a sampler that uses API. If you want to use this sampler in this notebook, please complete following two variables."
   ]
  },
  {
   "cell_type": "code",
   "id": "8a1012894e4440b4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:05:42.976686Z",
     "start_time": "2024-11-27T05:05:42.973530Z"
    }
   },
   "source": [
    "api_endpoint: str = 'api.bltcy.ai'  # the ip of your API provider, no \"https://\", such as \"api.bltcy.top\".\n",
    "api_key: str = ''  # your API key which may start with \"sk-......\"\n",
    "model: str = 'gpt-4o-mini'"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use our `HttpsApi` class to access to your API provider."
   ],
   "id": "d41e379fc7066c7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T05:05:46.852750Z",
     "start_time": "2024-11-27T05:05:46.849530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sampler = llm4ad.tools.llm.llm_api_https.HttpsApi(host=api_endpoint, key=api_key, model=model)"
   ],
   "id": "1ee25fc68833e4b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T05:05:51.268881Z",
     "start_time": "2024-11-27T05:05:50.410042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test the sampler\n",
    "response = sampler.draw_sample('Hello!')\n",
    "response"
   ],
   "id": "e20027b2a370b006",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "d27817cdec2cedfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Implement an evaluator and prepare a template program"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:05:54.121032Z",
     "start_time": "2024-11-27T05:05:54.111277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_valid_bin_indices(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns indices of bins in which item can fit.\n",
    "     Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\"\"\"\n",
    "    return np.nonzero((bins - item) >= 0)[0]\n",
    "\n",
    "\n",
    "def online_binpack(\n",
    "        items: tuple[float, ...], bins: np.ndarray, priority: callable\n",
    ") -> tuple[list[list[float, ...], ...], np.ndarray]:\n",
    "    \"\"\"Performs online binpacking of `items` into `bins`.\n",
    "     Args:\n",
    "        items: Tuple of items to be packed into bins.\n",
    "        bins: Array of capacities for each bin.\n",
    "        priority: Function that returns priority with which we want to add\n",
    "                  item to each bin.\n",
    "     Return:\n",
    "        packing: List of lists of items packed into each bin.\n",
    "        bins: Array of capacities for each bin.\"\"\"\n",
    "    # Track which items are added to each bin.\n",
    "    packing = [[] for _ in bins]\n",
    "    # Add items to bins.\n",
    "    for item in items:\n",
    "        # Extract bins that have sufficient space to fit item.\n",
    "        valid_bin_indices = get_valid_bin_indices(item, bins)\n",
    "        # Score each bin based on heuristic.\n",
    "        priorities = priority(item, bins[valid_bin_indices])\n",
    "        # Add item to bin with highest priority.\n",
    "        best_bin = valid_bin_indices[np.argmax(priorities)]\n",
    "        bins[best_bin] -= item\n",
    "        packing[best_bin].append(item)\n",
    "    # Remove unused bins from packing.\n",
    "    packing = [bin_items for bin_items in packing if bin_items]\n",
    "    return packing, bins\n",
    "\n",
    "\n",
    "def evaluate(instances: dict, priority: callable) -> float:\n",
    "    \"\"\"Evaluate heuristic function on a set of online binpacking instances.\n",
    "     Args:\n",
    "        instances: Dictionary of instances to evaluate on.\n",
    "        priority: Function that returns priority with which we want to add\n",
    "                  item to each bin.\n",
    "     Return:\n",
    "        float: Score of heuristic function.\"\"\"\n",
    "    # List storing number of bins used for each instance.\n",
    "    num_bins = []\n",
    "    # Perform online binpacking for each instance.\n",
    "    for name in instances:\n",
    "        instance = instances[name]\n",
    "        capacity = instance['capacity']\n",
    "        items = instance['items']\n",
    "        # Create num_items bins so there will always be space for all items,\n",
    "        # regardless of packing order. Array has shape (num_items,).\n",
    "        bins = np.array([capacity for _ in range(instance['num_items'])])\n",
    "        # Pack items into bins and return remaining capacity in bins_packed, which\n",
    "        # has shape (num_items,).\n",
    "        _, bins_packed = online_binpack(items, bins, priority)\n",
    "        # If remaining capacity in a bin is equal to initial capacity, then it is\n",
    "        # unused. Count number of used bins.\n",
    "        num_bins.append((bins_packed != capacity).sum())\n",
    "    # Score of heuristic function is negative of average number of bins used\n",
    "    # across instances (as we want to minimize number of bins).\n",
    "    return -np.mean(num_bins)"
   ],
   "id": "e52a0da7632055aa",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T05:06:00.321549Z",
     "start_time": "2024-11-27T05:06:00.318292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template_program = '''\n",
    "import numpy as np\n",
    "\n",
    "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns priority with which we want to add item to each bin.\n",
    "    Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\n",
    "    Return:\n",
    "        Array of same size as bins with priority score of each bin.\n",
    "    \"\"\"\n",
    "    return bins - item\n",
    "'''"
   ],
   "id": "c8ac0eda9094aafb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The evaluator defines how to evaluate the generated heuristic function. You should create a new Evaluator class by implementing `llm4ad.base.Evaluation`. You should override \"evaluate_program\" function to specify. Return None if the function is invalid.\n",
    "\n",
    "The `llm4ad.base.Evaluation` class provide acceleration and safe evaluation methods. You can use them by simply setting respective arguments. The commonly used two argument are:\n",
    "- `use_numba_accelerate`: If set to True, we will wrap the heuristic function with '@numba.jit(nopython=True)'. Please note that not all functions support numba.jit(), so use it appropriately.\n",
    "- `timeout_second`: Terminate the evaluation after timeout seconds. If set to `None`, the evaluator will wait until the evaluation finish.\n",
    "\n",
    "For more arguments, please refer to docstring in `llm4ad.base.Evaluation`."
   ],
   "id": "572b178366c60e8f"
  },
  {
   "cell_type": "code",
   "id": "3e3d88a87535b6b2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:06:04.294291Z",
     "start_time": "2024-11-27T05:06:04.288992Z"
    }
   },
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class OBPEvaluator(llm4ad.base.Evaluation):\n",
    "    \"\"\"Evaluator for online bin packing problem.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            use_numba_accelerate=True,\n",
    "            timeout_seconds=10,\n",
    "            template_program=template_program,\n",
    "        )\n",
    "        try:\n",
    "            with open('../online_bin_packing_fake/_data/weibull_train.pkl', 'rb') as f:\n",
    "                self._bin_packing_or_train = pickle.load(f)['weibull_5k_train']\n",
    "        except:\n",
    "            with open('/content/llm4ad/example/online_bin_packing_fake/_data/weibull_train.pkl', 'rb') as f:\n",
    "                self._bin_packing_or_train = pickle.load(f)['weibull_5k_train']\n",
    "\n",
    "    def evaluate_program(self, program_str: str, callable_func: callable) -> Any | None:\n",
    "        \"\"\"Evaluate a given function. You can use compiled function (function_callable),\n",
    "        as well as the original function strings for evaluation.\n",
    "        Args:\n",
    "            program_str: The function in string. You can ignore this argument when implementation.\n",
    "            callable_func: The callable Python function of your sampled heuristic function code. \n",
    "            You can call the program using 'program_callable(args..., kwargs...)'\n",
    "        Return:\n",
    "            Returns the fitness value. Return None if you think the result is invalid.\n",
    "        \"\"\"\n",
    "        # we call the _obp_evaluate.evaluate function to evaluate the callable code\n",
    "        return evaluate(self._bin_packing_or_train, callable_func)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "evaluator = OBPEvaluator()\n",
    "secure_evaluator = llm4ad.base.SecureEvaluator(evaluator=evaluator, debug_mode=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:06:27.472344Z",
     "start_time": "2024-11-27T05:06:27.466739Z"
    }
   },
   "id": "95d7221c858241d1",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test our evaluator"
   ],
   "id": "dfd37411a8e38ac1"
  },
  {
   "cell_type": "code",
   "id": "37bb26c049543591",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:06:37.617119Z",
     "start_time": "2024-11-27T05:06:34.875981Z"
    }
   },
   "source": [
    "# test the evaluator\n",
    "test_program = '''\n",
    "import numpy as np\n",
    "\n",
    "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns priority with which we want to add item to each bin.\n",
    "    Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\n",
    "    Return:\n",
    "        Array of same size as bins with priority score of each bin.\n",
    "    \"\"\"\n",
    "    return bins - item\n",
    "'''\n",
    "\n",
    "res = secure_evaluator.evaluate_program(test_program)\n",
    "print(res)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: evaluated program:\n",
      "import numba\n",
      "import numpy as np\n",
      "\n",
      "@numba.jit(nopython=True)\n",
      "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"Returns priority with which we want to add item to each bin.\n",
      "    Args:\n",
      "        item: Size of item to be added to the bin.\n",
      "        bins: Array of capacities for each bin.\n",
      "    Return:\n",
      "        Array of same size as bins with priority score of each bin.\n",
      "    \"\"\"\n",
      "    return bins - item\n",
      "\n",
      "-5000.0\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Terminating evaluation test\n",
    "Our platform will automatically terminate evaluation after `timeout_seconds` to prevent endless loop in the code. This is achieved by a secure evaluator."
   ],
   "id": "32bcf2a98d6c0d73"
  },
  {
   "cell_type": "code",
   "id": "1f5a41b77f092768",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:08:53.126537Z",
     "start_time": "2024-11-27T05:08:43.094371Z"
    }
   },
   "source": [
    "# we test an invalid program\n",
    "test_program = '''\n",
    "import numpy as np\n",
    "\n",
    "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns priority with which we want to add item to each bin.\n",
    "    Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\n",
    "    Return:\n",
    "        Array of same size as bins with priority score of each bin.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        pass\n",
    "'''\n",
    "\n",
    "res = secure_evaluator.evaluate_program(test_program)\n",
    "print(res)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: evaluated program:\n",
      "import numba\n",
      "import numpy as np\n",
      "\n",
      "@numba.jit(nopython=True)\n",
      "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"Returns priority with which we want to add item to each bin.\n",
      "    Args:\n",
      "        item: Size of item to be added to the bin.\n",
      "        bins: Array of capacities for each bin.\n",
      "    Return:\n",
      "        Array of same size as bins with priority score of each bin.\n",
      "    \"\"\"\n",
      "    while True:\n",
      "        pass\n",
      "\n",
      "DEBUG: the evaluation time exceeds 10s.\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "cc06b56e75ef7703",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Choose a method and run\n",
    "Our package support multiprocess running. However, the Colab backend has limited CPU support, so we set num_evlauators to 2."
   ]
  },
  {
   "cell_type": "code",
   "id": "d18cd55e162f0b94",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:08:58.934616Z",
     "start_time": "2024-11-27T05:08:58.900312Z"
    }
   },
   "source": [
    "from llm4ad.tools.profiler import ProfilerBase\n",
    "from llm4ad.method.randsample import RandSample\n",
    "\n",
    "# you can also try other LLM-EPS methods.\n",
    "rand_sample = RandSample(\n",
    "    llm=sampler,\n",
    "    profiler=ProfilerBase(log_dir='logs/randomsample', log_style='simple'),\n",
    "    evaluation=OBPEvaluation(),\n",
    "    max_sample_nums=10,\n",
    "    num_samplers=2,\n",
    "    num_evaluators=2,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================LLM Parameters===============================\n",
      "LLM: HttpsApi\n",
      "do_auto_trim: True\n",
      "debug_mode: False\n",
      "_host: api.bltcy.ai\n",
      "_key: sk-z4tSPqfaQ74KBpTlFc4f7fC767D04603Be0696F7A8BcC7D8\n",
      "_model: gpt-4o-mini\n",
      "_timeout: 20\n",
      "_kwargs: {}\n",
      "_cumulative_error: 0\n",
      "==================================Problem Parameters===============================\n",
      "Problem: OBPEvaluation\n",
      "task_description: Implement a function that returns the priority with which we want to add an item to each bin.\n",
      "use_numba_accelerate: False\n",
      "use_protected_div: False\n",
      "protected_div_delta: 1e-05\n",
      "random_seed: None\n",
      "timeout_seconds: 20\n",
      "exec_code: True\n",
      "safe_evaluate: True\n",
      "daemon_eval_process: False\n",
      "==================================Method Parameters===============================\n",
      "Method: RandSample\n",
      "_max_sample_nums: 10\n",
      "_num_samplers: 1\n",
      "_num_evaluators: 1\n",
      "_debug_mode: False\n",
      "_resume_mode: False\n",
      "_function_to_evolve_name: priority\n",
      "==================================End of Parameters===============================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "9fe4da616889236c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T05:09:58.826353Z",
     "start_time": "2024-11-27T05:09:10.238941Z"
    }
   },
   "source": [
    "if __name__ == '__main__':\n",
    "    rand_sample.run()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample1: Score=-2091.800     Cur_Best_Score=-2091.800\n",
      "Sample2: Score=None    Cur_Best_Score=-2091.800\n",
      "Sample3: Score=-5000.000     Cur_Best_Score=-2091.800\n",
      "Sample4: Score=-5000.000     Cur_Best_Score=-2091.800\n",
      "Sample5: Score=-5000.000     Cur_Best_Score=-2091.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x110e329d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/alevo311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mrand_sample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/llm4ad/llm4ad/method/randsample/randsample.py:178\u001B[0m, in \u001B[0;36mRandSample.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m# join all threads to the main thread\u001B[39;00m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_threads:\n\u001B[0;32m--> 178\u001B[0m     \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_profiler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_profiler\u001B[38;5;241m.\u001B[39mfinish()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/alevo311/lib/python3.11/threading.py:1119\u001B[0m, in \u001B[0;36mThread.join\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1116\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1119\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[1;32m   1122\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[1;32m   1123\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/alevo311/lib/python3.11/threading.py:1139\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1138\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1139\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1140\u001B[0m         lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m   1141\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "89874fa892f009df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
