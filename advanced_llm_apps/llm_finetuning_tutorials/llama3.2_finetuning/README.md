## 游붗 Ajuste Fino de Llama 3.2 en 30 L칤neas de Python

Este script demuestra c칩mo realizar el ajuste fino (finetune) del modelo Llama 3.2 utilizando la biblioteca [Unsloth](https://unsloth.ai/), que hace el proceso f치cil y r치pido. Puedes ejecutar este ejemplo para realizar el ajuste fino de los modelos Llama 3.1 1B y 3B de forma gratuita en Google Colab.

### Caracter칤sticas

- Realiza el ajuste fino del modelo Llama 3.2 utilizando la biblioteca Unsloth
- Implementa Adaptaci칩n de Bajo Rango (LoRA) para un ajuste fino eficiente
- Utiliza el conjunto de datos FineTome-100k para el entrenamiento
- Configurable para diferentes tama침os de modelo (1B y 3B)

### Instalaci칩n

1. Clona el repositorio:

```bash
git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git
cd awesome-llm-apps/llm_finetuning_tutorials/llama3.2_finetuning
```

2. Instala las dependencias requeridas:

```bash
pip install -r requirements.txt
```

## Uso

1. Abre el script en Google Colab o en tu entorno Python preferido.

2. Ejecuta el script para iniciar el proceso de ajuste fino:

```bash
# Ejecuta el script completo
python finetune_llama3.2.py
```

3. El modelo ajustado se guardar치 en el directorio "finetuned_model".

## C칩mo Funciona

1. **Carga del Modelo**: El script carga el modelo Llama 3.2 3B Instruct utilizando FastLanguageModel de Unsloth.

2. **Configuraci칩n de LoRA**: Se aplica Adaptaci칩n de Bajo Rango a capas espec칤ficas del modelo para un ajuste fino eficiente.

3. **Preparaci칩n de Datos**: Se carga el conjunto de datos FineTome-100k y se preprocesa utilizando una plantilla de chat.

4. **Configuraci칩n del Entrenamiento**: El script configura el SFTTrainer con argumentos de entrenamiento espec칤ficos.

5. **Ajuste Fino**: El modelo se ajusta con el conjunto de datos preparado.

6. **Guardado del Modelo**: El modelo ajustado se guarda en el disco.

## Configuraci칩n

Puedes modificar los siguientes par치metros en el script:

- `model_name`: Cambia a "unsloth/Llama-3.1-1B-Instruct" para el modelo 1B
- `max_seq_length`: Ajusta la longitud m치xima de la secuencia
- `r`: Rango de LoRA
- Hiperpar치metros de entrenamiento en `TrainingArguments`

## Personalizaci칩n

- Para utilizar un conjunto de datos diferente, reemplaza la llamada a la funci칩n `load_dataset` con tu conjunto de datos deseado.
- Ajusta los `target_modules` en la configuraci칩n de LoRA para ajustar diferentes capas del modelo.
- Modifica la plantilla de chat en `get_chat_template` si est치s utilizando un formato conversacional diferente.

## Ejecuci칩n en Google Colab

1. Abre un nuevo cuaderno de Google Colab.
2. Copia el script completo en una celda de c칩digo.
3. Agrega una celda al principio para instalar las bibliotecas requeridas:

```
!pip install torch transformers datasets trl unsloth
```

4. Ejecuta las celdas para iniciar el proceso de ajuste fino.

## Notas

- Este script est치 optimizado para ejecutarse en el nivel gratuito de Google Colab, que proporciona acceso a GPU.
- El proceso de ajuste fino puede llevar alg칰n tiempo, dependiendo del tama침o del modelo y los recursos computacionales disponibles.
- Aseg칰rate de tener suficiente espacio de almacenamiento en tu instancia de Colab para guardar el modelo ajustado.