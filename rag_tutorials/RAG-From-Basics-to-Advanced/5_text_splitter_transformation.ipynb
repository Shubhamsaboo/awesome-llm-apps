{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Transformation using Text Splitters**\n",
    "\n",
    "LLMs have fixed maximum context window. If you document contains more token then the maximum context window size, LLMs won't be able to process it. So, it is important to break the documents into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Text Splitters**\n",
    "\n",
    "LangChain offers many different types of text splitters. These all live in the `langchain-text-splitters` package. Below is a table listing all of them, along with a few characteristics:\n",
    "> **Name:** Name of the text splitter  \n",
    "> **Splits On:** How this text splitter splits text  \n",
    "> **Adds Metadata:** Whether or not this text splitter adds metadata about where each chunk came from.  \n",
    "> **Description:** Description of the splitter, including recommendation on when to use it.\n",
    "\n",
    "| Name | Splits On | Adds Metadata | Description |\n",
    "| :--- | :--- | :---: | :--- |\n",
    "| Character | A user defined character | ❌ | Splits text based on a user defined character. One of the simpler methods. |\n",
    "| Recursive | A list of user defined characters | ❌ | Recursively splits text. Splitting text recursively serves the purpose of trying to keep related pieces of text next to each other. This is the recommended way to start splitting text. |\n",
    "| HTML | HTML specific characters | ✅ | Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML) |  \n",
    "| Markdown | Markdown specific characters | ✅ | Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown) |  \n",
    "| Code | Code (Python, JS) specific characters | ❌ | Splits text based on characters specific to coding languages. 15 different languages are available to choose from. |\n",
    "| Token | Tokens | ❌ | Splits text on tokens. There exist a few different ways to measure tokens. |\n",
    "| [Experimental] Semantic Chunker | Sentences | ❌ | First splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg Kamradt |\n",
    "\n",
    "**credits:** [ThatAIGuy GitHub Repository](https://github.com/bansalkanav/Generative-AI-Scratch-2-Advance-By-ThatAIGuy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./example_data/text/file_1.txt\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n"
     ]
    }
   ],
   "source": [
    "doc_content = data[0].page_content\n",
    "\n",
    "print(doc_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. CharacterTextSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "char_text_split = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "char_chunks = char_text_split.split_text(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of texts variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'str'>\n",
      "\n",
      "Total number of documents inside texts list: 2\n",
      "\n",
      "* Content of first chunk: This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "\n",
      "* Content of second chunk: Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of texts variable:\", type(char_chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(char_chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside texts list:\", len(char_chunks))\n",
    "print()\n",
    "print(\"* Content of first chunk:\", char_chunks[0])\n",
    "print()\n",
    "print(\"* Content of second chunk:\", char_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "for chunk in char_chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multiple documents with Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 106.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('example_data/text', glob=\"*.txt\", show_progress=True, loader_cls=TextLoader)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Data Variable:  <class 'list'>\n",
      "Number of Documents: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of Data Variable: \", type(data))\n",
    "\n",
    "print(\"Number of Documents:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in data:\n",
    "#     print(doc.page_content)\n",
    "#     print(\"---\")\n",
    "\n",
    "# for doc in data:\n",
    "#     print(doc.metadata)\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_contents = [doc.page_content for doc in data]\n",
    "doc_metadata = [doc.metadata for doc in data]\n",
    "\n",
    "doc_chunks = char_text_split.create_documents(doc_contents, doc_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents inside chunks: 3\n",
      "\n",
      "Document 1 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 1 chunks: This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 2 chunks: Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 metadata: {'source': 'example_data\\\\text\\\\file_2.txt'}\n",
      "Document 3 chunks: These were unbelievely expensive\n",
      "and he'll grow out of them\n",
      "\n",
      "in 20 minutes,\n",
      "but I couldn't resist!\n",
      "\n",
      "Look at these.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents inside chunks:\", len(doc_chunks))\n",
    "print()\n",
    "for i, chunk in enumerate(doc_chunks, start=1):\n",
    "    print(f\"Document {i} metadata: {chunk.metadata}\")\n",
    "    print(f\"Document {i} chunks: {chunk.page_content}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Recursively split by character**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_text_splitter.create_documents(doc_contents, doc_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents inside chunks: 3\n",
      "\n",
      "Document 1 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 1 chunks: This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 2 chunks: Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 metadata: {'source': 'example_data\\\\text\\\\file_2.txt'}\n",
      "Document 3 chunks: These were unbelievely expensive\n",
      "and he'll grow out of them\n",
      "\n",
      "in 20 minutes,\n",
      "but I couldn't resist!\n",
      "\n",
      "Look at these.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents inside chunks:\", len(recursive_chunks))\n",
    "print()\n",
    "for i, chunk in enumerate(recursive_chunks, start=1):\n",
    "    print(f\"Document {i} metadata: {chunk.metadata}\")\n",
    "    print(f\"Document {i} chunks: {chunk.page_content}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Split by tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. NLTK Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91889\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "nltk_text_splitter = NLTKTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "nltk_chunks = nltk_text_splitter.create_documents(doc_contents, doc_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents inside chunks: 5\n",
      "\n",
      "Document 1 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 1 chunks: This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 2 chunks: Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 3 chunks: And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 metadata: {'source': 'example_data\\\\text\\\\file_2.txt'}\n",
      "Document 4 chunks: These were unbelievely expensive\n",
      "and he'll grow out of them\n",
      "\n",
      "in 20 minutes,\n",
      "but I couldn't resist!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5 metadata: {'source': 'example_data\\\\text\\\\file_2.txt'}\n",
      "Document 5 chunks: Look at these.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents inside chunks:\", len(nltk_chunks))\n",
    "print()\n",
    "for i, chunk in enumerate(nltk_chunks, start=1):\n",
    "    print(f\"Document {i} metadata: {chunk.metadata}\")\n",
    "    print(f\"Document {i} chunks: {chunk.page_content}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii. tiktoken Text Splitter**\n",
    "\n",
    "`tiktoken` is a fast BPE tokenizer created by `OpenAI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "tiktoken_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "tiktoken_chunks = tiktoken_text_splitter.create_documents(doc_contents, doc_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents inside chunks: 3\n",
      "\n",
      "Document 1 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 1 chunks: This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 2 chunks: Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 metadata: {'source': 'example_data\\\\text\\\\file_2.txt'}\n",
      "Document 3 chunks: These were unbelievely expensive\n",
      "and he'll grow out of them\n",
      "\n",
      "in 20 minutes,\n",
      "but I couldn't resist!\n",
      "\n",
      "Look at these.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents inside chunks:\", len(tiktoken_chunks))\n",
    "print()\n",
    "for i, chunk in enumerate(tiktoken_chunks, start=1):\n",
    "    print(f\"Document {i} metadata: {chunk.metadata}\")\n",
    "    print(f\"Document {i} chunks: {chunk.page_content}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii. Sentence Transformers Token Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91889\\anaconda3\\envs\\geminienv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters.sentence_transformers import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "st_text_splitter = SentenceTransformersTokenTextSplitter(model_name=\"sentence-transformers/all-mpnet-base-v2\", \n",
    "                                                         chunk_size=100, \n",
    "                                                         chunk_overlap=50)\n",
    "\n",
    "st_chunks = st_text_splitter.create_documents(doc_contents, doc_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents inside chunks: 2\n",
      "\n",
      "Document 1 metadata: {'source': 'example_data\\\\text\\\\file_1.txt'}\n",
      "Document 1 chunks: this is pretty much what ' s happened so far. ross was in love with rachel since forever. every time he tried to tell her, something got in the way iike cats, italian guys. and finally, chandler was, like, \" forget about her. \"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 metadata: {'source': 'example_data\\\\text\\\\file_2.txt'}\n",
      "Document 2 chunks: these were unbelievely expensive and he ' ll grow out of them in 20 minutes, but i couldn ' t resist! look at these.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents inside chunks:\", len(st_chunks))\n",
    "print()\n",
    "for i, chunk in enumerate(st_chunks, start=1):\n",
    "    print(f\"Document {i} metadata: {chunk.metadata}\")\n",
    "    print(f\"Document {i} chunks: {chunk.page_content}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geminienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
