##  Llama-3.1 Local con RAG
Aplicaci贸n Streamlit que te permite chatear con cualquier p谩gina web utilizando Llama-3.1 local y Generaci贸n Aumentada por Recuperaci贸n (RAG). Esta aplicaci贸n se ejecuta completamente en tu computadora, lo que la hace 100% gratuita y sin necesidad de conexi贸n a internet.


### Caracter铆sticas
- Ingresa la URL de una p谩gina web
- Haz preguntas sobre el contenido de la p谩gina web
- Obt茅n respuestas precisas utilizando RAG y el modelo Llama-3.1 ejecut谩ndose localmente en tu computadora

### 驴C贸mo Empezar?

1. Clona el repositorio de GitHub

```bash
git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git
cd awesome-llm-apps/rag_tutorials/llama3.1_local_rag
```
2. Instala las dependencias requeridas:

```bash
pip install -r requirements.txt
```
3. Ejecuta la Aplicaci贸n Streamlit
```bash
streamlit run llama3.1_local_rag.py
```

### 驴C贸mo Funciona?

- La aplicaci贸n carga los datos de la p谩gina web utilizando WebBaseLoader y los divide en fragmentos utilizando RecursiveCharacterTextSplitter.
- Crea embeddings de Ollama y un almac茅n de vectores utilizando Chroma.
- La aplicaci贸n configura una cadena RAG (Generaci贸n Aumentada por Recuperaci贸n), que recupera documentos relevantes basados en la pregunta del usuario.
- Se llama al modelo Llama-3.1 para generar una respuesta utilizando el contexto recuperado.
- La aplicaci贸n muestra la respuesta a la pregunta del usuario.

