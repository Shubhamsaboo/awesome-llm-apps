# 6.2 LLM Interaction Callbacks

This tutorial demonstrates how to use `before_model_callback` and `after_model_callback` to monitor LLM requests and responses.

## ğŸ¯ Learning Objectives

- Understand LLM interaction callbacks
- Learn how to monitor LLM requests and responses
- Track token usage and response times
- Estimate API costs
- Monitor LLM performance metrics

## ğŸ“ Project Structure

```
6_2_llm_interaction_callbacks/
â”œâ”€â”€ agent.py          # Agent with LLM interaction callbacks
â”œâ”€â”€ app.py            # Streamlit web interface
â”œâ”€â”€ requirements.txt  # Python dependencies
â””â”€â”€ README.md         # This file
```

## ğŸ”§ Setup

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up API key:**
   ```bash
   # Create .env file
   echo "GOOGLE_API_KEY=your_api_key_here" > .env
   ```

## ğŸš€ Running the Demo

### Command Line Demo
```bash
python agent.py
```

### Web Interface
```bash
streamlit run app.py
```

## ğŸ§  Core Concept: LLM Interaction Monitoring

LLM interaction callbacks allow you to monitor the communication between your agent and the underlying language model, providing insights into requests, responses, and performance metrics.

### **LLM Interaction Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Input    â”‚â”€â”€â”€â–¶â”‚  LLM Request    â”‚â”€â”€â”€â–¶â”‚  LLM Response   â”‚
â”‚                 â”‚    â”‚   Callback      â”‚    â”‚   Callback      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                       â”‚
                              â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Model API      â”‚    â”‚  Token Usage    â”‚
                       â”‚  (Gemini)       â”‚    â”‚  & Performance  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Callback Execution Timeline**

```
Timeline: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶

User Message
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ before_model    â”‚ â† Records start time, model info
â”‚ _callback       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM API Call    â”‚ â† Actual request to Gemini
â”‚ (Gemini 3 Flash)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ after_model     â”‚ â† Calculates duration, tokens, cost
â”‚ _callback       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Response to User
```

## ğŸ“– Code Walkthrough

### **1. Callback Functions**

The callbacks work in pairs to monitor the complete LLM interaction:

**Before Callback (`before_model_callback`):**
- Extracts model information from `llm_request`
- Records request timestamp
- Stores data in session state for after callback
- Logs request details (model, time, agent)

**After Callback (`after_model_callback`):**
- Retrieves stored request data from session state
- Calculates response duration
- Extracts token usage from `llm_response.usage_metadata`
- Estimates API costs based on token count
- Logs performance metrics

### **2. State Management Between Callbacks**

```
Session State Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ before_callback â”‚â”€â”€â”€â–¶â”‚  Session State  â”‚â”€â”€â”€â–¶â”‚ after_callback  â”‚
â”‚ stores:         â”‚    â”‚                 â”‚    â”‚ retrieves:      â”‚
â”‚ - start_time    â”‚    â”‚ - llm_request_  â”‚    â”‚ - start_time    â”‚
â”‚ - model         â”‚    â”‚   time          â”‚    â”‚ - model         â”‚
â”‚ - prompt_length â”‚    â”‚ - llm_model     â”‚    â”‚ - prompt_length â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **3. Agent Setup**

The agent is configured with both callbacks:
- `before_model_callback`: Monitors request initiation
- `after_model_callback`: Monitors response completion
- Uses `InMemoryRunner` for proper callback triggering

## ğŸ§ª Testing Examples

### **Example Output Format**

```
ğŸ¤– LLM Request to gemini-3-flash-preview
â° Request time: 19:15:30
ğŸ“‹ Agent: llm_monitor_agent

ğŸ“ LLM Response from gemini-3-flash-preview
â±ï¸ Duration: 1.45s
ğŸ”¢ Tokens: 156
ğŸ’° Estimated cost: $0.0004

```

### **What Each Metric Tells You**

- **â° Request time**: When the LLM request was initiated
- **â±ï¸ Duration**: Total time from request to response
- **ğŸ”¢ Tokens**: Total tokens consumed (input + output)
- **ğŸ’° Estimated cost**: Approximate API cost based on token usage

## ğŸ” Key Concepts

### **LLM Request Monitoring**
- **Model Information**: Track which model is being used
- **Timing**: Record request timestamps
- **State Management**: Store request data for response analysis

### **LLM Response Monitoring**
- **Response Time**: Calculate duration from request to response
- **Token Usage**: Track total tokens consumed
- **Cost Estimation**: Approximate API costs

### **Usage Metadata**
- **Token Count**: `llm_response.usage_metadata.total_token_count`
- **Model Information**: Available in request and response
- **Timing Data**: Stored in session state between callbacks

## ğŸ¯ Use Cases

- **Performance Monitoring**: Track LLM response times
- **Cost Management**: Monitor API usage and costs
- **Quality Assurance**: Analyze prompt and response patterns
- **Debugging**: Troubleshoot LLM interaction issues
- **Analytics**: Collect usage statistics and metrics

## ğŸš¨ Common Mistakes

1. **Incorrect callback signatures:**
   ```python
   # âŒ Wrong
   def before_model_callback(context, model, prompt):
   
   # âœ… Correct
   def before_model_callback(callback_context: CallbackContext, llm_request):
   ```

2. **Wrong token extraction:**
   ```python
   # âŒ Wrong
   tokens = llm_response.usage_metadata.get('total_token_count')
   
   # âœ… Correct
   tokens = getattr(llm_response.usage_metadata, 'total_token_count', 0)
   ```

3. **Not using InMemoryRunner:**
   ```python
   # âŒ Wrong - callbacks won't trigger
   agent.run(message)
   
   # âœ… Correct
   runner.run_async(...)
   ```

## ğŸ”— Next Steps

- Try Tutorial 6.3: Tool Execution Callbacks
- Experiment with different cost estimation models
- Add response quality metrics
- Implement rate limiting and quota management
- Create custom analytics dashboards 